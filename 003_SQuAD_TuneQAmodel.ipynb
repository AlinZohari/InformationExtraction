{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlinZohari/InformationExtraction/blob/main/003_SQuAD_TuneQAmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ha78KH0frS0"
      },
      "source": [
        "# Fine-Tuning QA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg2fF-z4AVgI"
      },
      "source": [
        "This notebook are run in Google Colab to leverage its GPU capability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss08hKKUfrS1"
      },
      "source": [
        "Reference:\n",
        "1. Hugging Face -  [Question and Answering Task Guide](https://huggingface.co/docs/transformers/tasks/question_answering)\n",
        "2. Creating Train and Validation Datasets - https://simpletransformers.ai/docs/qa-data-formats/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9OAMAN7Qf0r"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39dkGpjcQfn4"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNDA2y8dqyJc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_SRPedNAVgN"
      },
      "source": [
        "Setting CUDA_LAUNCH_BLOCKING=1 makes all CUDA operations synchronous, which means the CPU will wait for the GPU to finish before executing the next line of code. This makes it easier to identify and debug errors, because the stack trace will show exactly where the error occurred.However, this will make the code run slower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO6fnBssfrS1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRJ6UjpMQTDv"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8SFujquQTDw"
      },
      "outputs": [],
      "source": [
        "!pip show accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-knfpWYLQTDx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYzVPh-bAVgS"
      },
      "source": [
        "## Pretrained model capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u03rN-SyQTDy"
      },
      "source": [
        "let us see first the capability of the pretrained deepset/roberta-base-squad2 model on our questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hif7qErQTDz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForQuestionAnswering\n",
        "\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
        "model = RobertaForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# Read context from a .txt file\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/AlinZohari/InformationExtraction/main/data/authorize_doc/Kuiper_FCC-20-102A1.txt\"\n",
        "response = requests.get(url)\n",
        "context = response.text\n",
        "\n",
        "# Dictionary of questions\n",
        "questions = {\n",
        "    \"const_name\": \"What's the name of the satellite constellation the company seeks to deploy or operate?\",\n",
        "    \"date_release\": \"On which date was the document released?\",\n",
        "    \"date_50\": \"By which date must the company launch and operate half of its satellites?\",\n",
        "    \"date_100\": \"By which date is the company expected to have all its satellites operational?\",\n",
        "    \"total_sat_const\": \"How many satellites is the company authorized to deploy and operate for this constellation?\",\n",
        "    \"altitude\": \"At which authorized altitudes will the company deploy its satellites?\",\n",
        "    \"inclination\": \"What are the authorized satellite inclinations within the corresponding altitudes?\",\n",
        "    \"number_orb_plane\": \"How many orbital planes, corresponding to given altitudes and inclinations, has the company been authorized for?\",\n",
        "    \"total_sat_per_orb_plane\": \"How many satellites are allocated to each orbital plane?\",\n",
        "    \"total_sat_per_alt_incl\": \"How many satellites, for each altitude and inclination, are there across all matching orbital planes?\",\n",
        "    \"operational_lifetime\": \"What is the satellite's expected operational lifetime in years?\"\n",
        "}\n",
        "\n",
        "# Loop through each question\n",
        "for key, question in questions.items():\n",
        "    # Prepare the input\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "\n",
        "    # Get the model's prediction\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    output = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    answer_start_scores = output.start_logits\n",
        "    answer_end_scores = output.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores)\n",
        "    answer = tokenizer.decode(input_ids[0][answer_start:answer_end + 1])\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_RmyiZiQTD0"
      },
      "source": [
        "The warning message you are seeing is due to the truncation strategy used by the tokenizer. The 'longest_first' truncation strategy truncates tokens from the longest of the two sequences (question or context) until they fit within the specified max_length. The warning is informing you that the overflowing tokens, which are the tokens removed during truncation, are not being returned in the inputs. This is expected behavior, as we are not using the overflowing tokens in this case.\n",
        "\n",
        "The answers that are just indicate that the model is not able to find a suitable answer in the context for the given question. This could be because the answer is not present in the context, or because the context is too large and the relevant portion was truncated.\n",
        "\n",
        "Because of this let us fine tune this model to fit our purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWcQQAvpAVgT"
      },
      "source": [
        "## Lets now Fine-Tuned the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgacgFFyAVgT"
      },
      "source": [
        "We are using deepset/roberta-base-squad2 model which is used for question answering taks. It is based oon RoBERTa model which ia a variant of BERT (Bidirectional Encoder Representations from Transformers) model. BERT and RoBERTa are models designed to understand the context and relationships among words.\n",
        "- RoBERTa: RoBERTa stands for \"A Robustly Optimized BERT Pretraining Approach\". It is an optimized version of BERT, which means it is trained on more data and for more iterations than BERT. RoBERTa modifies key hyperparameters in BERT, including removing the next-sentence pretraining objective, and training with much larger mini-batches and learning rates.\n",
        "- squad2: SQuAD stands for Stanford Question Answering Dataset version 2.0 an extension of SQuAD 1.1 which includes unanswerable questions. This means that the model trained on this dataset not only needs to answer questions but also has to determine if a question is answerable or not based on the provided context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaW5Ky5gfrS2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "#Reference: https://huggingface.co/deepset/roberta-base-squad2\n",
        "\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "#Load model & tokenizer\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "#using AutoModelForQuestionAnswering automatically infer the correct model and tokenizer classes to use based on the model name. This makes the code more flexible as it can work with any model architecture\n",
        "#using RobertaTokenizerFast which is a fast tokenizer for RoBERTa models. The \"fast\" tokenizers are implemented in Rust and are more performant compared to the standard Python tokenizers. They also provide additional functionalities like alignment between the original and tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iap32VtEfrS2"
      },
      "outputs": [],
      "source": [
        "#looking at RoBerta Question Answering\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82U5kIUNfrS3"
      },
      "source": [
        "How to fine-tune a QA model\n",
        "- we need GPU\n",
        "- building a training script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJnj5WCtfrS3"
      },
      "outputs": [],
      "source": [
        "#getting our own build training datasets\n",
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/AlinZohari/InformationExtraction/main/data/QA_model/train.json\"\n",
        "response = requests.get(url)\n",
        "train = response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L3zc7epfrS3"
      },
      "outputs": [],
      "source": [
        "#looking at the train dataset\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tff-Cb8QgfHF"
      },
      "outputs": [],
      "source": [
        "##etting our own build validation datasets\n",
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/AlinZohari/InformationExtraction/main/data/QA_model/validation.json\"\n",
        "response = requests.get(url)\n",
        "validation = response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alciwDQ-giqR"
      },
      "outputs": [],
      "source": [
        "#looking atthe validation dataset\n",
        "validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToeQAFFFQTD4"
      },
      "source": [
        "## Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwgN7dj8YVAy"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF4JPX4_qFKj"
      },
      "outputs": [],
      "source": [
        "#we need to defined the tokenizer\n",
        "#from transformers import RobertaTokenizerFast\n",
        "#tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
        "# needed to use BertTokenizerFast/ RobertaTokenizerFast return_offset_mapping feature is not available when using Python tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23GIl1VwQTD4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = []\n",
        "    contexts = []\n",
        "    answers = []\n",
        "\n",
        "    for i in range(len(examples['context'])):\n",
        "        context = examples['context'][i]\n",
        "        qas = examples['qas'][i]\n",
        "\n",
        "        for qa in qas:\n",
        "            questions.append(qa['question'].strip())\n",
        "            contexts.append(context)\n",
        "            if not qa['is_impossible']:\n",
        "                ans = qa['answers'][0]\n",
        "                answers.append({'answer_start': [ans['answer_start']], 'text': [ans['text']]})\n",
        "            else:\n",
        "                answers.append({'answer_start': [None], 'text': [None]})\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer['answer_start'][0]\n",
        "        end_char = start_char + len(answer['text'][0]) if answer['text'][0] else None\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        if start_char is None or end_char is None:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            idx = 0\n",
        "            while sequence_ids[idx] != 1:\n",
        "                idx += 1\n",
        "            context_start = idx\n",
        "            while sequence_ids[idx] == 1:\n",
        "                idx += 1\n",
        "            context_end = idx - 1\n",
        "\n",
        "            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "                start_positions.append(0)\n",
        "                end_positions.append(0)\n",
        "            else:\n",
        "                idx = context_start\n",
        "                while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                    idx += 1\n",
        "                start_positions.append(idx - 1)\n",
        "\n",
        "                idx = context_end\n",
        "                while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                    idx -= 1\n",
        "                end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Convert lists to Dataset objects\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame(train))\n",
        "validation_dataset = Dataset.from_pandas(pd.DataFrame(validation))\n",
        "\n",
        "# Apply preprocess_function\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_validation = validation_dataset.map(preprocess_function, batched=True, remove_columns=validation_dataset.column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYCmaOz4ojpU"
      },
      "outputs": [],
      "source": [
        "tokenized_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OBWD7eaopKk"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZYcbJZnZHAV"
      },
      "source": [
        "The DefaultDataCollator is a class from the transformers library that is used to collate samples into batches for training or evaluation. When you train a model, you usually don't pass the entire dataset at once, but rather use mini-batches of data. The data_collator is responsible for taking the individual samples and combining them into these mini-batches.\n",
        "\n",
        "The DefaultDataCollator will:\n",
        "\n",
        "Handle the padding of the input data (if necessary) to ensure that all samples in the batch have the same length.\n",
        "Convert the batch into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfacp8LHfrS4"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_PFghoLfrS4"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSvspLvFfrS4"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNAQ5ldHm88T"
      },
      "source": [
        "metric = load_metric(\"squad\") loads the SQuAD (Stanford Question Answering Dataset) evaluation metric. This metric computes the Exact Match (EM) and F1 score, which are commonly used for evaluating question answering models.\n",
        "\n",
        "Exact Match (EM): This is the simplest metric. It measures the percentage of predictions that match any one of the ground truth answers exactly.\n",
        "F1 Score: This is a more complex metric that considers the overlap between the prediction and ground truth answer. It is the harmonic mean of precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAZoA65GfrS4"
      },
      "outputs": [],
      "source": [
        "#defining training argument\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfZ3oP87nNkB"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "metric = load_metric(\"squad\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    # Get the model's predictions\n",
        "    start_logits, end_logits = p.predictions\n",
        "    start_preds = np.argmax(start_logits, axis=1)\n",
        "    end_preds = np.argmax(end_logits, axis=1)\n",
        "\n",
        "    # Get the ground truth labels\n",
        "    start_labels = p.label_ids[0]\n",
        "    end_labels = p.label_ids[1]\n",
        "\n",
        "    # Convert the predictions and labels to the format expected by the metric\n",
        "    predictions = [{'prediction_text': tokenizer.decode(input_ids[start:end+1].tolist())} for input_ids, start, end in zip(tokenized_validation['input_ids'], start_preds, end_preds)]\n",
        "    references = [{'answers': {'answer_start': [answer['answer_start']], 'text': [answer['text']]}} for answer in tokenized_validation['answers']]\n",
        "\n",
        "    # Compute the metric\n",
        "    result = metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    return result\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_validation,\n",
        "    data_collator=data_collator,\n",
        "    #compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k2177J8awbN"
      },
      "source": [
        "The TrainOutput object contains some information about the training process:\n",
        "\n",
        "global_step: The total number of training steps completed. This is 18 in your case.\n",
        "training_loss: The final training loss. This is 3.016 in your case.\n",
        "metrics: A dictionary containing some additional metrics:\n",
        "train_runtime: The total runtime of the training in seconds. This is 11.33 seconds in your case.\n",
        "train_samples_per_second: The number of samples processed per second. This is 11.118 in your case.\n",
        "train_steps_per_second: The number of training steps completed per second. This is 1.588 in your case.\n",
        "total_flos: The total number of floating-point operations performed during training. This is 24,692,543,511,552 in your case.\n",
        "train_loss: The final training loss. This is the same as training_loss and is 3.016 in your case.\n",
        "epoch: The total number of epochs completed. This is 3 in your case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp-FLBMsn2lP"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1RIOow1a3aF"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"/content/gdrive/MyDrive/tuned_model\")\n",
        "tokenizer.save_pretrained(\"/content/gdrive/MyDrive/tuned_model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV3jVGbAbsDN"
      },
      "source": [
        "## Using tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ju2zkd9dgym"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the directory\n",
        "os.listdir('/content/gdrive/MyDrive/tuned_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a-Wmm7Zdn9Y"
      },
      "outputs": [],
      "source": [
        "print(os.path.abspath('/content/gdrive/MyDrive/tuned_model'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ_Tef9nbEwE"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, RobertaTokenizerFast, RobertaForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "#model = RobertaForQuestionAnswering.from_pretrained(\"/content/gdrive/MyDrive/tuned_model\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"/content/gdrive/MyDrive/tuned_model\")\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"/content/gdrive/MyDrive/tuned_model\")\n",
        "\n",
        "# Read context from a .txt file\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/AlinZohari/InformationExtraction/main/data/authorize_doc/Kuiper_FCC-20-102A1.txt\"\n",
        "response = requests.get(url)\n",
        "context = response.text\n",
        "\n",
        "#define the questions\n",
        "questions = [\n",
        "    \"What's the name of the satellite constellation the company seeks to deploy or operate?\",\n",
        "    \"On which date was the document released?\",\n",
        "    \"By which date must the company launch and operate half of its satellites?\",\n",
        "    \"By which date is the company expected to have all its satellites operational?\",\n",
        "    \"How many satellites is the company authorized to deploy and operate for this constellation?\",\n",
        "    \"At which authorized altitudes will the company deploy its satellites?\",\n",
        "    \"What are the authorized satellite inclinations within the corresponding altitudes?\",\n",
        "    \"How many orbital planes, corresponding to given altitudes and inclinations, has the company been authorized for?\",\n",
        "    \"How many satellites are allocated to each orbital plane?\",\n",
        "    \"How many satellites, for each altitude and inclination, are there across all matching orbital planes?\",\n",
        "    \"What is the satellite's expected operational lifetime in years?\"\n",
        "]\n",
        "\n",
        "# Function to ask a single question\n",
        "def ask_question(question, context):\n",
        "    # Split the context into chunks of 512 tokens\n",
        "    chunk_size = 512 - tokenizer.num_special_tokens_to_add(pair=True)\n",
        "    context_chunks = [context[i:i+chunk_size] for i in range(0, len(context), chunk_size)]\n",
        "\n",
        "    answers = []\n",
        "\n",
        "    for context_chunk in context_chunks:\n",
        "        inputs = tokenizer(question, context_chunk, return_tensors='pt')\n",
        "        outputs = model(**inputs)\n",
        "        answer_start = torch.argmax(outputs.start_logits)\n",
        "        answer_end = torch.argmax(outputs.end_logits)\n",
        "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end+1]))\n",
        "        answers.append(answer)\n",
        "\n",
        "    # Combine the answers from each chunk\n",
        "    full_answer = ' '.join(answers)\n",
        "\n",
        "    return full_answer\n",
        "\n",
        "# Ask each question\n",
        "answers = [ask_question(question, context) for question in questions]\n",
        "\n",
        "# Print the answers\n",
        "for question, answer in zip(questions, answers):\n",
        "    print(f'Question: {question}')\n",
        "    print(f'Answer: {answer}\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFcp_XQEl9ER"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
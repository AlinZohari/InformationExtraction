{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation with Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=SW1ZdqH0rRQ&t=872s <br>\n",
    "https://www.youtube.com/watch?v=6ppxd9lp-X0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing LLM model and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "\n",
    "from getpass import getpass\n",
    "from secret_key import huggingface_key\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = huggingface_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://huggingface.co/deepset/roberta-base-squad2\"\"\"\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing module for langchain and kor\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "from kor.extraction import create_extraction_chain\n",
    "from kor.nodes import Object, Text, Number\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from kor import extract_from_documents, from_pydantic, create_extraction_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing llm model\n",
    "\n",
    "#from langchain.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id = model,\n",
    "    huggingfacehub_api_token = huggingface_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  and Splitting Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert filename you want to extract information from \n",
    "filename = \"data/authorize_doc/StarlinkGen2_FCC-22-91A1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the document\n",
    "def import_document(filename):\n",
    "    encodings = ['utf-8', 'ISO-8859-1', 'utf-16', 'ascii', 'cp1252']\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            with open(filename, 'r', encoding=enc) as file:\n",
    "                document_text = file.read()\n",
    "            return document_text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{filename}' not found.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while importing the document: {e}\")\n",
    "            return None\n",
    "    print(f\"Error: Could not decode file with any of the tried encodings: {encodings}\")\n",
    "    return None\n",
    "\n",
    "document = import_document(filename)\n",
    "if document is not None:\n",
    "    print(\"Document content:\")\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly explore the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is token count in textacy\n",
    "import textacy\n",
    "doc = textacy.make_spacy_doc(document, lang=\"en_core_web_sm\")\n",
    "print(doc._.preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy import text_stats as ts\n",
    "\n",
    "# Number of words and number of unique words\n",
    "print(\"Number of words: \", ts.n_words(doc))\n",
    "print(\"Number of unique words: \", ts.n_unique_words(doc))\n",
    "\n",
    "# Entropy of words in the document- measures how much informations produced on the average of the word\n",
    "print(\"Entropy: \", ts.entropy(doc))\n",
    "\n",
    "# Compute the Type-Token Ratio (TTR) of doc_or_token,a direct ratio of the number of unique words (types) of all words (token)\n",
    "print(\"Diversity: \", ts.diversity.ttr(doc))\n",
    "\n",
    "# Flesch Kincaid grade level: readability tests designed to indicate how difficult a passage is\n",
    "print(\"Flesch Kincaid: \",ts.flesch_kincaid_grade_level(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split the document into chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the document into chunks\n",
    "doc = Document(page_content = document)\n",
    "split_docs = RecursiveCharacterTextSplitter().split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pydantic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Kor doumentation it says that the Validation doe NO imply that extraction was correct.\n",
    "Validation only implies that the data was returned in the correct shape and meets all validation criteria.\n",
    "This does not mean that the LLM didn't make up some information\n",
    "\n",
    "we can use pydantic to sip the invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from typing import Optional, List, Union\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrbitEnv(BaseModel):\n",
    "    const_name: str = Field(\n",
    "        description=\"The satellite constellation name for which the company applied to deploy or operate\",\n",
    "    )\n",
    "    orbit_type: str = Field(\n",
    "        description=\"The orbit type into which the satellites will be launched\"\n",
    "    )\n",
    "    application: str = Field(\n",
    "        description=\"The application or services that the satellites would provide\"\n",
    "    )\n",
    "    date_50: str = Field(\n",
    "        description=\"The date when the company is order to deploy and operate half of its satellites\"\n",
    "    )\n",
    "    date_100: str = Field(\n",
    "        description=\"The date when the company is order to deploy and operate all of its remaining satellites.\"\n",
    "    )\n",
    "    total_sat_const: int = Field(\n",
    "        description=\"The concluding total number of satellites that the company has been authorized to deploy and operate for the constellation\"\n",
    "    )\n",
    "    altitude: Optional[List[float]]= Field(\n",
    "        description=\"The granted altitudes of the satellites that the company has been authorized to deploy\"\n",
    "    )\n",
    "    #can add to description: that the inclination would be in degree - maybe delete the words altitude\n",
    "    #The granted inclination of the satellites that the company has been authorized to deploy, in degree\n",
    "    inclination: Optional[List[float]] = Field(\n",
    "        description=\"The granted inclination of the satellites that the company has been authorized to deploy, respective to the altitudes\"\n",
    "    )\n",
    "    number_orb_plane: Optional[List[int]] = Field(\n",
    "        description=\"The number of orbital planes, respective to the altitudes and inclination, that the company has been authorized to deploy\"\n",
    "    )\n",
    "    total_sat_per_orb_plane: Optional[List[int]]= Field(\n",
    "        description=\"The specific count of satellites located in each individual orbital plane. This count refers to the total number of satellites within one orbital plane, and it can vary from plane to plane based on the altitude and inclination, and if not mentioned in text, 'total_sat_per_alt_incl' divide by 'number_orb_plane' will give this value\"\n",
    "    )\n",
    "    total_sat_per_alt_incl: Optional[List[int]] = Field(\n",
    "        description=\"The total number of satellites at a specific altitude and inclination across all orbital planes sharing these characteristics. This count represents the overall number of satellites with the specified altitude and inclination parameters, and if not mentioned in the text, the multiplication of 'number_orb_plane' and 'total_sat_per_orb_plane' will give this value\"\n",
    "    )\n",
    "    orbit_shape: Optional[str] = Field(\n",
    "        description=\"The shape of the orbital plane whether its circular, elliptical or are not mention in the document\"\n",
    "    )\n",
    "    operational_lifetime : Optional[str] = Field(\n",
    "        description=\"The operational lifetime of the satellite in the constellation in years\"\n",
    "    )\n",
    "\n",
    "\n",
    "    @validator(\"const_name\", \"orbit_type\", \"application\")\n",
    "    def validate_name(cls, v):\n",
    "        if not re.match(\"^[a-zA-Z\\s().,-]*$\", v):\n",
    "            raise ValueError(\"The field can only contain alphabetic characters, spaces, parentheses, periods, commas and hyphen.\")\n",
    "        return v\n",
    "    \n",
    "    @validator(\"total_sat_const\", \"number_orb_plane\", \"total_sat_per_orb_plane\", \"total_sat_per_alt_incl\", \"operational_lifetime\")\n",
    "    def validate_whole_number(cls, v):\n",
    "        if isinstance(v, list):\n",
    "            if not all(isinstance(i, int) for i in v):\n",
    "                raise ValueError(\"All elements of the list must be whole numbers.\")\n",
    "        elif v is not None and not isinstance(v, int):\n",
    "            raise ValueError(\"The field must be a whole number.\")\n",
    "        return v\n",
    "\n",
    "    @validator(\"altitude\", \"inclination\")\n",
    "    def validate_number(cls, v):\n",
    "        if isinstance(v, list):\n",
    "            if not all(isinstance(i, (int, float)) for i in v):\n",
    "                raise ValueError(\"All elements of the list must be numbers (integer or decimal).\")\n",
    "        elif v is not None and not isinstance(v, (int, float)):\n",
    "            raise ValueError(\"The field must be a number (integer or decimal).\")\n",
    "        return v\n",
    "\n",
    "    @validator(\"orbit_shape\")\n",
    "    def validate_orbit_shape(cls, v):\n",
    "        if not re.match(\"^[a-zA-Z\\s]*$\", v):\n",
    "            raise ValueError(\"orbit_shape can only contain alphabetic characters and spaces.\")\n",
    "        return v\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Optional[str]' means that the field can be either a str(string) or None effectively making it optional\n",
    "\n",
    "'str' means that it is mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema, extraction_validator = from_pydantic(\n",
    "    OrbitEnv,\n",
    "    description=\"Extract the Orbital Environment information of a Satellite Constellation from the authorized document. Include details such as the company name, orbit type, application, dates for 50 percent and 100 percent satellite launches, total number of authorized satellites, altitude, inclination, number of orbital planes, number of satellites per plane, and orbit shape\",\n",
    "    examples=[\n",
    "        (\n",
    "            \"\"\"In this Order and Authorization, we grant, to the extent set forth below, the request of Kuiper Systems LLC (Kuiper or Amazon) to deploy a non-geostationary satellite orbit (NGSO) system to provide service using certain Fixed-Satellite Service (FSS).\n",
    "                Operating 3,372 satellites in 102 orbital planes at altitudes of 590 km, 610 km, and 630 km in a circular orbit.\n",
    "                At 590 km, 30 orbital planes with 28 satellites per plane for a total of 840 satellites at inclination of 33 degree.\n",
    "                At 610 km, 42 orbital planes with 36 satellites per plane for a total of 1512 satellites at inclination of 42 degree.\n",
    "                At 630 km, 30 orbital planes with 34 satellites per plane for a total of 1020 satellite at inclination of 51.9 degree.\n",
    "                The constellation are require to launch and operate 50 percent of its satellites no later than July 30, 2026, and Kuiper must launch the remaining space stations necessary to complete its authorized service constellation, place them in their assigned orbits, and operate each of them in accordance with the authorization no later than July 30, 2029.\"\"\",\n",
    "                \n",
    "            {\"const_name\": \"Kuiper System LLC\", \"orbit_type\": \"non-geostationary satellite orbit (NGSO)\", \"application\": \"Fixed-Satellite Service (FSS)\", \"date_50\": \"July 30, 2026\", \"date_100\": \"July 30, 2029\", \"total_sat_const\": 3372, \"altitude\": [590, 610, 630],  \"inclination\": [33, 42, 51.9], \"number_orb_plane\": [30, 42, 30], \"total_sat_per_orb_plane\": [28, 36, 34], \"total_sat_per_alt_incl\": [840, 1512, 1020], \"orbit_shape\": \"circular\"}\n",
    "        ),\n",
    "        (\n",
    "            \"Iridium must launch 50 percent of the maximum number of proposed space stations, place them in the assigned orbits, and operate them in accordance with this grant no later than November 12,2028, and must launch the remaining space stations necessary to complete its authorized service constellation, place them in their assigned orbits, and operate them in accordance with the authorization no later than May 16,2030.\",\n",
    "            {\"const_name\": \"Iridium\",\"date_50\":\"November 12,2028,\",\"date_100\":\"May 16,2030\"}\n",
    "        ),\n",
    "        (\n",
    "            \"They must launch 50 percent of the maximum number of proposed space stations, place them in the assigned orbits, and operate them in accordance with this grant of U.S. market access no later than December 31,1989, and must launch the remaining space stations necessary to complete its authorized service constellation, place them in their assigned orbits, and operate them in accordance with the grant of U.S. market access no later than December 21,1997.\",\n",
    "            {\"date_50\":\"December 31,1989\",\"date_100\":\"November 21,1997\"}\n",
    "        ),\n",
    "        (\n",
    "            \"In this Order and Declaratory Ruling, we grant in part and defer in part the petition for declaratory ruling of WorldVu Satellites Limited (OneWeb) for modification of its grant of U.S. market access for a its satellite constellation authorized by the United Kingdom. As modified, the constellation will operate with four fewer satellites, reduced from 720 to 716 satellites.\",\n",
    "            {\"const_name\": \"WorldVu Satellites Limited (OneWeb)\", \"total_sat_const\": 716}\n",
    "        ),\n",
    "        (\n",
    "            \"They sought Commission approval for a non-geostationary satellite orbit (NGSO) system to provide fixed-satellite service (FSS) in the United States.\",\n",
    "            {\"orbit_type\": \"non-geostationay satellite orbit (NGSO)\", \"application\": \"fixed-satellite service (FSS)\"}\n",
    "        ),\n",
    "        (\n",
    "            \"\"\"The proposed Telesat system is set to feature a robust constellation of 124 satellites.\n",
    "            A set of six orbital planes, each inclined at 99.5 degrees, will host nine satellites per plane at an approximate altitude of 1,000 kilometers.\n",
    "            Additionally, seven more orbital planes, each tilted at 37.4 degrees, will carry another group of satellites, with each plane accommodating ten satellites at a higher altitude of approximately 1,248 kilometers.\n",
    "            It's noteworthy that all satellites will occupy a circular orbit, ensuring systematic and efficient coverage.\"\"\",\n",
    "            {\"const_name\": \"Telesat\", \"total_sat_const\": 124, \"altitude\": [1000, 1248], \"inclination\": [99.5, 37.4], \"number_orb_plane\": [6, 7], \"total_sat_per_orb_plane\": [9, 10], \"total_sat_per_alt_incl\": [54, 70], \"orbit_shape\": \"circular\"}\n",
    "        ),\n",
    "        #different between total_sat_per_orb_plane and total_sat_per_alt_incl\n",
    "        (\n",
    "            \"20 orbital planes with 28 satellites per plane for a total of 560 satellites at inclination of 33 degree will be placed at an altitude approximately 800 km.\",\n",
    "            {\"altitude\": 800, \"inclination\": 33, \"number_orb_plane\": 20, \"total_sat_per_orb_plane\": 28, \"total_sat_per_alt_incl\": 560}\n",
    "        ),\n",
    "        #total_sat_per_alt_incl = number_orb_plane x total_sat_per_orb_plane\n",
    "        (\n",
    "            \"8 orbital plane containing 15 satellites each which are inclined at 56 degree with altitude of 700 kilometers\",\n",
    "            {\"altitude\": 700, \"inclination\": 56, \"number_orb_plane\": 8, \"total_sat_per_orb_plane\": 15, \"total_sat_per_alt_incl\": 120}\n",
    "        ),\n",
    "        #total_sat_per_orb_plane = total_sat_per_alt_incl x number_orb_plane\n",
    "        (\n",
    "            \"72 of the satellites will be distributed equally and place at 6 orbital planes, which are inclined 99.5 degrees, satellites will be at an approximate altitude of 1,000 kilometers\",\n",
    "            {\"altitude\": 1000, \"inclination\": 99.5, \"number_orb_plane\": 6, \"total_sat_per_orb_plane\": 12, \"total_sat_per_alt_incl\": 72}\n",
    "        ),\n",
    "        #operational_lifetime\n",
    "        (\n",
    "            \"The operational lifetime for the satellite in the constellation in 10 years\",\n",
    "            {\"operational_lifetime\": 10}\n",
    "        ),\n",
    "\n",
    "    ],\n",
    "    many=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will provide more examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" #maneuverable and spin-stabilized\n",
    "(\n",
    "    \"\"\"Each satellite in the constellation is equipped with propulsion, enabling it to perform maneuvers to avoid collisions and navigate to its designated operational orbit.\n",
    "    Additionally, the satellites also have spin stabilizers, ensuring their stability during orbital operation.\"\"\",\n",
    "    {\"maneuverable\": \"y\", \"spin_stabilized\": \"y\"}\n",
    "), \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'many' parameter determine whether the funciton should expect to work with a single instance of the object or multiple instances\n",
    "\n",
    "- If many=False (the default), the schema expects to validate a single object of the class defined in the function call (OrbitEnv in your case).\n",
    "- If many=True, the schema expects to validate a list of objects of the class defined in the function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_extraction_chain(\n",
    "    llm,\n",
    "    schema,\n",
    "    encoder_or_encoder_class=\"json\",\n",
    "    validator=extraction_validator,\n",
    "    input_formatter=\"triple_quotes\",\n",
    ")\n",
    "\n",
    "#csv does support list , but json is not as accurate as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at what are the prompt and istruction pass to the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.prompt.format_prompt(text=\"[user input]\").to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seeing how much it cost us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    document_extraction_results = await extract_from_documents(\n",
    "        chain, split_docs, max_concurrency=5, use_uid=False, return_exceptions=True\n",
    "    )\n",
    "    #split_docs is where we input the document we want to extract\n",
    "    #use_uid: parameter that determine whether or not to use a unique identifier (uid)when processesing document\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Successful Requests: {cb.successful_requests}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seeing the raw return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_extraction_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_dataframe(json_data):\n",
    "    # Prepare an empty list to store all OrbitEnv data\n",
    "    data = []\n",
    "\n",
    "    for record in json_data:\n",
    "        # Check if the record is a dictionary. If not, print an error and continue to the next record\n",
    "        if not isinstance(record, dict):\n",
    "            print(f\"Error encountered: {record}\")\n",
    "            continue\n",
    "        \n",
    "        orbitenv_list = record.get('data', {}).get('orbitenv', [])\n",
    "        for orbitenv in orbitenv_list:\n",
    "            data.append([\n",
    "                orbitenv.get('const_name', ''),\n",
    "                orbitenv.get('orbit_type', ''),\n",
    "                orbitenv.get('application', ''),\n",
    "                orbitenv.get('date_50', ''),\n",
    "                orbitenv.get('date_100', ''),\n",
    "                orbitenv.get('total_sat_const', ''),\n",
    "                orbitenv.get('altitude', '') or '',\n",
    "                orbitenv.get('inclination', '') or '',\n",
    "                orbitenv.get('number_orb_plane', '') or '',\n",
    "                orbitenv.get('total_sat_per_orb_plane', '') or '',\n",
    "                orbitenv.get('total_sat_per_alt_incl', '') or '',\n",
    "                orbitenv.get('orbit_shape', ''),\n",
    "                orbitenv.get('operational_lifetime', '')\n",
    "            ])\n",
    "\n",
    "    # Convert the list into a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['constellationName', 'orbitType', 'application','date50', 'date100', 'totalSatelliteNumber', 'altitudes','inclination', 'numberOrbPlane', 'totalSatellitePerOrbPlane','totalSatellitePerAltIncl', 'orbShape', 'operationalLifetime'])\n",
    "\n",
    "    # Replace various values with None\n",
    "    df.replace(['','-',0,'Null', 'null', 'Not Mentioned', 'Not mentioned', 'not mentioned', 'unknown', 'Unknown','N/A'], None, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage:\n",
    "df = generate_dataframe(document_extraction_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent in Each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def find_most_frequent(df: pd.DataFrame) -> dict:\n",
    "    most_frequent_dict = {}\n",
    "    for column in df.columns:\n",
    "        column_without_none = df[column].dropna()\n",
    "        if not column_without_none.empty:\n",
    "            mode = column_without_none.mode()\n",
    "            if len(mode) > 1:\n",
    "                most_frequent_dict[column] = {\"message\": \"Multiple modes found\", \"modes\": mode.tolist()}\n",
    "            else:\n",
    "                most_frequent_dict[column] = mode[0]\n",
    "        else:\n",
    "            most_frequent_dict[column] = None\n",
    "    return most_frequent_dict\n",
    "\n",
    "def convert(o):\n",
    "    if isinstance(o, np.generic):\n",
    "        return o.item()\n",
    "    raise TypeError\n",
    "\n",
    "def convert_to_json(data: dict) -> str:\n",
    "    try:\n",
    "        json_data = json.dumps(data, default=convert)\n",
    "        return json_data\n",
    "    except TypeError:\n",
    "        return json.dumps({\"error\": \"Failed to serialize data\"})\n",
    "\n",
    "result = find_most_frequent(df)\n",
    "result\n",
    "#returninng dictionary key-value pair, mutable , can be add, remove, change element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do and if statement here to find date_500 and date_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Json and exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = convert_to_json(result)\n",
    "\n",
    "name = result.get('constellationName', {}).get('modes', [None])[0] if isinstance(result.get('constellationName', {}), dict) else result.get('constellationName', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name is not None:\n",
    "    name = re.sub(r'\\W+', '_', name)\n",
    "    filename = f'output/{name}_{model}_data.json'\n",
    "\n",
    "    with open(filename, 'w+') as txt_file:\n",
    "        txt_file.write(json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yet to do\n",
    "- for the total number of satellite in constellation you can do if there are multiple mode found match the value with the sum of all array in total number of satellite (per altitude/inclination) - if it match that is your total number of satellite in constellation\n",
    "- adding in maneuverable and spin stabilisation as a field and operational lifetime - manueverable and stabilisation is really hard to get it right - also add orbit epoch\n",
    "- using different LLM model\n",
    "- using different company order authorize document -done\n",
    "- using different type of document - schedule S or techical document\n",
    "- how to measure validation (intrisic and extrinsic) - validate if the extracted info is in and at what paragraph - or hallucination\n",
    "- make this a model? - putting input text - output json fresh  - calculation coding - true process all column 29\n",
    "\n",
    "- maybe before doing extraction - do a sentiment analysis for the whole document - to see if the purpose constallation are fully granted, partially granted ot denied \n",
    "\n",
    "- need to extract also the release date\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

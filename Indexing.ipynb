{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing with Retrieval QA -with Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=inAY6M6UUkk <br>\n",
    "https://www.youtube.com/watch?v=cVA1RPsGQcw - embedding and vector db pinecone <br>\n",
    "https://www.youtube.com/watch?v=DXmiJKrQIvg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/authorize_doc/Kuiper_FCC-20-102A1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the document\n",
    "def import_document(filename):\n",
    "    encodings = ['utf-8', 'ISO-8859-1', 'utf-16', 'ascii', 'cp1252']\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            with open(filename, 'r', encoding=enc) as file:\n",
    "                document_text = file.read()\n",
    "            return document_text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{filename}' not found.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while importing the document: {e}\")\n",
    "            return None\n",
    "    print(f\"Error: Could not decode file with any of the tried encodings: {encodings}\")\n",
    "    return None\n",
    "\n",
    "document = import_document(filename)\n",
    "if document is not None:\n",
    "    print(\"Document content:\")\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking and overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Document with the content\n",
    "doc = Document(page_content=document)\n",
    "\n",
    "# Create the text splitter with specific parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,           # Check if this parameter is valid\n",
    "    chunk_overlap=100,        # Check if this parameter is valid\n",
    "    length_function=len,      # Check if this parameter is valid\n",
    "    keep_separator=True       # This is a valid parameter as per the traceback\n",
    ")\n",
    "\n",
    "# Split the document into smaller chunks\n",
    "split_docs = text_splitter.split_documents([doc])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we see here the document is separate into chunks and are overlapping \n",
    "split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating embedding for all the document chunks using OpenAI \"ada\" and store in Pinecone Vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeing what gpt model are available for us\n",
    "openai.Model.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding model \n",
    "embeddings = OpenAIEmbeddings(modelName=\"text-embedding-ada-002\")\n",
    "\n",
    "#finding the dimension\n",
    "query_result = embeddings.embed_query(\"MEOW_OW_OW_000\")\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we use pinecone as a vector store. set the dimension to 1536 and metric to cosine similarity\n",
    "other vectore store can use Chroma, custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_env = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "if not pinecone_api_key or not pinecone_env:\n",
    "    raise ValueError(\"Environment variables not set.\")\n",
    "\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n",
    "\n",
    "index_name = \"indexing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index if it doesn't exist\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(name=index_name, metric='cosine', dimension=len(query_result))\n",
    "\n",
    "# Store documents in Pinecone\n",
    "docsearch = Pinecone.from_documents(split_docs, embeddings, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can check you pinecone now to see if the vector are in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the chunk with questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not set.\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,  # Don't be creative and make up an answer\n",
    "    request_timeout=120,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the retriever\n",
    "# Only retrieve documents that have a relevance score above a certain threshold\n",
    "#retriever = docsearch.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8})\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "retriever = docsearch.as_retriever(search_kwargs={'k': 2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESOURCE: creating a chain, retrie ver, custom prompt- https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa\n",
    "We can also modify the search by passing specific search arguments through the retriever to the search function, using the search_kwargs keyword argument.\n",
    "\n",
    "- k defines how many documents are returned; defaults to 4.\n",
    "- score_threshold allows you to set a minimum relevance for documents returned by the retriever, if you are using the \"similarity_score_threshold\" search type.\n",
    "- fetch_k determines the amount of documents to pass to the MMR algorithm; defaults to 20.\n",
    "- lambda_mult controls the diversity of results returned by the MMR algorithm, with 1 being minimum diversity and 0 being maximum. Defaults to 0.5.\n",
    "- filter allows you to define a filter on what documents should be retrieved, based on the documents' metadata. This has no effect if the Vectorstore doesn't store any metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the prompt \n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the template without direct variable embedding\n",
    "prompt_template = \"\"\"\n",
    "Follow exactly those 3 steps:\n",
    "1. Read the context below and aggregate this data\n",
    "Context: {context}\n",
    "2. Answer the question using only this context\n",
    "3. Show the source for your answers\n",
    "User Question: {question}\n",
    "\n",
    "If you don't have any context and are unsure of the answer, reply that you don't know about this topic.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs=chain_type_kwargs, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of questions\n",
    "questions = {\n",
    "    \"const_name\": \"What's the name of the satellite constellation the company seeks to deploy or operate?\",\n",
    "    \"date_release\": \"On which date was the document released?\",\n",
    "    \"date_50\": \"By which date must the company launch and operate half of its satellites?\",\n",
    "    \"date_100\": \"By which date is the company expected to have all its satellites operational?\",\n",
    "    \"total_sat_const\": \"How many satellites is the company authorized to deploy and operate for this constellation?\",\n",
    "    \"altitude\": \"At which authorized altitudes will the company deploy its satellites?\",\n",
    "    \"inclination\": \"What are the authorized satellite inclinations within the corresponding altitudes?\",\n",
    "    \"number_orb_plane\": \"How many orbital planes, corresponding to given altitudes and inclinations, has the company been authorized for?\",\n",
    "    \"total_sat_per_orb_plane\": \"How many satellites are allocated to each orbital plane?\",\n",
    "    \"total_sat_per_alt_incl\": \"How many satellites, for each altitude and inclination, are there across all matching orbital planes?\",\n",
    "    \"operational_lifetime\": \"What is the satellite's expected operational lifetime in years?\"\n",
    "}\n",
    "\n",
    "# Store the results\n",
    "results_content = {}\n",
    "#Store the source_document\n",
    "source_documents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, query in questions.items():\n",
    "    matched_docs = retriever.get_relevant_documents(query)\n",
    "    result = qa({\"context\": matched_docs, \"query\": query})\n",
    "    results_content[key] = result['result']\n",
    "    source_documents[key] = result['source_documents']  # Assuming the key in the result is 'source_documents'\n",
    "\n",
    "for key, answer in results_content.items():\n",
    "    print(f\"\\n## Question ({key}): {questions[key]}\\n\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(f\"Source Documents: {source_documents[key]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
